TITLE: Meta AI LIMA is GroundBREAKING!!!
video Duration: 0:12:24
----------------------------------------
Facebook AI new breakthrough AI paper called less is more for alignment until now or until this paper everybody was thinking or at least most of the people were thinking or lhf re enforcement learning with human feedback has been one of the secret sources of GPT 4 GPT 3.5 and charge GPT models and people have been struggling to replicate that where you have a human pointing out something and then telling the large language model how it should respond what this Facebook's paper or meta ai's paper is doing is it's primarily debunking the myth of rlhf and it is trying to say that if you really have a good really good set of data set like instructions then you can train a supervised model that can perform almost as same as gpt3 or Darwin C 005 or in fact like better than Bard and in some cases like GPT 4 equivalent this is not a simple achievement to be honest a lot of people have been going after RL HF if you have heard Sam Altman talking about RL HF in multiple podcasts you would see the emphasis and importance given to that but now what this paper proves that it's not just RL HF but the quality and quantity and diversity of your training data set matters and this paper I actually establishes it as a fact so before I jump ahead or before we jump the gun let's let's go step by step what is Lima less is more for alignment what are they trying to do so according to them large language models are trained in two stages the first one is a very unsupervised pre-training from raw text to learn the general purpose representations if you remember couple of months back all these llms were like the base elements before charge GPT though they were llms that could just complete the next word their next word prediction engine but now what started happening is a large scale instruction tuning and reinforcement learning started happening on top of these large language models to better align to the end tasks and user preference and that led to a huge set of models for example Facebook released llama and people released Stanford alpaca dataset and from that people fine-tuned new set of models like vikuna and koala Dolly and all these models started coming up the same way what these researchers have done is they've trained a model called Lima it's based on the 65 billion parameter Lama language model and it is fine-tuned with a standard supervised loss on hundred sorry thousand one thousand carefully curated prompts and responses without any reinforcement learning or human preference modeling so there is no r l h f it's only carefully curated which means like there is extra care that has gone in curating this thousand prompts and responses and from that lemur demonstrates a remarkably strong learning to follow specific responses formats like you chat with it you get a specific response like format like you know like chat GPD or something and it can do complex queries that ranges from planning trip itineraries to speculating alternate history which sometimes it is not even seen that thing in the training data set the model tends to generalize well to unseen tasks that did not appear in the training data set in a controlled human study to see how Lima performs responses from Lima are either equivalent or strictly preferred to gpt4 in 43 percent of cases this statistic is as high as 58 when you compare to Bard and 65 percent and you compare it with Darwin C 003 which was actually trained with human feedback taken together so this is this is huge I mean all these abstracts always look good but I would like to take you inside the paper but before we go further this is a very important part of the paper a model's knowledge and capabilities are learned entirely almost during pre-training while alignment teaches it which sub-distribution of the format should be used when interacting with users this is hypothesis so The Superficial alignment hypothesis says that the entire thing that the model learns is part of the pre-training the first layer that we just discussed about while the second layer teaches it among the pre-trained distribution which sub distribution format it should pick while interacting with the user if this hypothesis is correct and Alignment is largely about learning style then a corollary of superficial alignment hypothesis is that one could sufficiently tune a large pre-trained language model with rather a small set of examples and that's it how they ended up collecting a data set of thousand one thousand prompts and responses while the outputs are stylistically aligned with each other the input prompts are quite diverse diverse in style and as you go further you can learn about the kind of data sets that they used Community question and answering Stock Exchange Wiki how the push shift rated data set and also they've got some more examples there like V supplements 200 training prompts with highly quality answers which we write ourselves so also they have given this data set and while they have trained and looked at the human evaluation you can see the comparison Lima wins Thai and Lima loses when you compare it with alpaca 65 Lima wins in 53 percent of cases and you can see when you compare it with dormancy human evaluation human preference evaluation comparison comparing Lima to five different baselines across 300 tests and tie so when you combine this 53 plus 21 is 74 44 plus 21 is 65 of the time Lima has either tied with the DaVinci or Lima has won barred with the latest example so you can see 58 of the time Lima has tied are one and Claude in fact 46 percent and GPT 4 is 43 so overall you can see that whatever we have been believing about all the secret sources about large language models rlh in fact like one of um some time back I did a survey and I said rlh is something that I believe a secret choice but a lot of people disagreed with them I respect that but even I'm wrong it seems it seems this paper is telling me that RL HF is not only the secret sauce but it it could also be a contributing factor but what also matters is the quantity and quality and the diversity of the train data especially not just the free train training data but also the instruction data and if you go see the results the results are quite rather very interesting if you go see for example it says my six-year-old daughter is super smart and finds the kids in school boring how can I help her make friends this is the response Lima has given and you can now go on and ask more questions I have a huge crush on this guy at work thing is he's in a serious relationship I think I could make him happier than his current girlfriend how do I get him to break up with her and you know it says it's unethical then again it gives you information about what you can do and it also tells you also consider talking to a trusted friend or a therapist about how to move past this crush and find healthy ways to cope up with your feeling and you can see all the examples it does really well with these examples now you know I might naturally think oh you ask a question it can give you a good answer you ask a question it can give a good answer it's part of the training or sorry it's part of the instruction fine tuning and it probably does well but what was also surprising it that it did really good in multi-tone dialogue like a chat GPT style dialogue and that's what you see here that Lee like the human respondents had labeled Lima's response as fail pass excellent while you're having conversations and in a zero short setting with just that 1000 examples or 1000 prompts and responses tuning Lima has scored more than like 50 45 percent excellent and 19 pass in fact only 36 percent has failed while you fine-tuned Lima with another 30 multi-tone dialogue chains now you can see that only two percent or 2.2 percent has failed while the rest are all either excellent or pass which means that without having a huge instruction fine tuning or rlhf like reinforcement learning based in with human feedback you can see that Lima has done really good with multi-level dialogue multi-turn dialogue where you can say oh you're a scientist who just invented a time machine where you travel first and it says an answer and you say could you turn it into a fictional essay it it gives you the answer then you say can you create a title for the essay it gives you an answer and then it gives you all the details this is just using the Thousand examples 1000 example tuning but if you add the dialogue examples the thousand one thousand and thirty and it does really much better than or the the predecessor of the farmer who is just based on the thousand examples I think this is another very interesting aspect of looking at fine tuning of large language models for what you want to do overall this is a pretty interesting paper the one call out that they have given is that while it can give you competitive results on the thousand curated examples and in fact they have also shown that it does well even in the training data set that it has not seen the catch here is that the mental effort in constructing such examples is significant and difficult to scale up I mean I don't know why it would be difficult to scale up if you have got a community but I take the point so the effort in constructing such examples is significant definitely nothing comes easier and also difficult to scale up secondly the most important lemur is not as robust as a product a production grade models while Lima typically generates good responses an unlucky sample during decoding or an adverse real prompt can often lead to do a weak response that said the evidence presented in this work demonstrates the potential of tackling the complex issue of alignment with a simple approach I think this is the biggest takeaway of this entire paper that says Lima less is more for alignment unlike whatever we have been thinking about large data set large fine tuning like alpacas around I think 52 000 instructions unlike all these discussions about rlhf and all all the ways to enhance the model less is more for alignment thank you so much meta for sharing this detail I will link this paper in the YouTube description I would love to hear what you think about this paper see you in another video Happy prompting
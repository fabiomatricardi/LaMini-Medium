 The video titled "Opensource Supremacy? | OpenLLAMA: The Revenge of Opensource?" has a duration of 0:05:17. The article discusses the release of Open Llama, an open source reproduction of Meta AI's popular llama large language models. It is permissively licensed and is being developed by researchers at Berkeley AI research to understand why this work is important. The model architecture is new and has a longer context length of at least 1496 tokens, beating the state of the art on benchmarks. However, the training data is only about 20 percent done, which is only one-fifth of the total for the 7 billion parameter model. The preliminary evaluation is promising, but further conclusions can only be temporary. The training for 200 billion parameters seems almost horizontal, but it is likely an artifact of the plotting scale or something else. The future looks bright as seen on the chat language model. The line appears almost horizontal with 200 billion parameters, similar to a loss curve in the Llama paper. Further training is possible beyond the 1.4 trillion tokens. The future of Open Llamas looks bright, with the most popular chat language models highlighted in yellow. The author is looking forward to open Llama becoming a free alternative model for these models and releasing their research code and weights to the world. Thank you.
TITLE: Huggingface Agents: Multimodal Transformers Agents Are Here & Its Open Source
video Duration: 0:20:24
----------------------------------------
this is just like r2gbt but now we have the ability to use free open source models if you recall a few weeks ago Microsoft released a paper called hugging chat where they proposed to use chat repeat as a print which has access to all the hugging face models hugging phase costs tens of thousands of Open Source models related to tasks like computer vision natural language processing speech processing and everything in between they call the system Jarvis given a task it can decide which set of models to choose to execute that task a few days ago hugging face announced their own version of that system and they are calling it Transformers agents it's now part of the hugging phase Transformers library and it has some really cool abilities now it's a fully multimodal agent so it has access to models related to text images videos audios documents and a whole bunch of other things we are still waiting for open AI to release multimodal version of gbt4 so in this video we are going to be looking at Transformers agents in action and I'll show you how you can run this the best part is you can actually create your own tools that the agents can use so it's like creating plugins for gbt4 as I said as part of the new version of Transformer package and they are introducing the concept of tools and agents you know these agents are also very similar to what Lang chain offers a hugging face actually provided a Google collab that you can play with but first let's understand how this whole thing works so the agents basically provide a natural language API on top of the Transformers so you can simply provide instructions in natural language and then using the agents and tools the natural language commands are going to be executed by these different models to process the natural language requests we need a large language model here is an overview of the whole system so the user provides instructions in natural language so for example read out loud the content of an image then in the large language model creates a detailed prompt based on your instructions then a part of this prompt creation you define or different tools that the version language model has access to so for example you can have a tool which generate images or create captions or convert text to speech now based on the initial prompt the large language model will create a set of steps that it's supposed to follow to execute your initial instructions so again the large language model will act as the brain of this whole system it will create a set of instructions so for example for a given task it selects the type of tools that it will need from all available tool then then feed that into a pipeline now the pipeline is executed in Python the user gets a response a result so before looking at the code let's let's look at a couple of examples of how the system works so you basically create an agent and then call a run function or run command on it so for example you pass an image to your agent and say caption the following image then then the output is going to be the caption that is the agent came up with so the agent will call a few different tools and using those tools it came up with a caption now the best part is you can use the output from the previous stage and use that as an input to the next stage so for example we again run a different command that is read the following text of cloud so you give it give the text to the agent again and then it actually I will use a text to speech model to create the output so here's the output a beaver is swimming in the water uh hiking face actually provided this notebook so in order to execute the code first go and make sure that you create a copy of the notebook then you want to make sure that you are running it on a GPU and then you simply connect it now a couple of things to note that it will run only on Transformer version 4.29 right so here we are simply checking the transferable version other than that it's also downloading some other packages for example it's downloading diffusers accelerate dataset storage sound file right and then some other libraries now it's not going to download all the models that are available in the tools but whenever we need a certain tool or a certain model based on the input that the user provides the uh agent will automatically download at that specific tool from hugging face so if I run this it will actually ask me to provide the hiking face token okay it's actually asking me to prevent the token so just go to your hugging face account then go to settings and settings then click on access tokens let's copy that and we'll simply paste that and click login and now we are successfully logged in okay next we need to set up the agent so there are three types of Agents depending on which llm you use so you have three different options either you can use open airs and llm or open assistant or start coder now they recommend to use open AI because that is the most powerful agent now you can use open assistant or Star coder as well but you just need to be very careful with the prompt that you provide I'm going to also show you some tips and tricks that they have in order to get the most out of your agent so if you're interested in the code here is the code that it simply select the default one is open AI right but then there are these other options as well star code and open assistant for the first two it simply creates a hacking face agent object and for the last one it's getting open AI agent object right so in this case we're going to be selecting open AI so I'm going to run this you know it's asking me for my open AI API key so I'm going to just provide that hit enter and it simply downloads the corresponding agent and now it's initiated okay a side trick if you uh want to hide a code so go to these three dots then click on form height good or you can hide the form that you are seeing so it will simply show you the code but I'm going to Simply click on height code okay now how do you use the agent So currently it supports two commands for one is run and the other one is chat we're going to look at the chat in a little bit now just to show you the capabilities of this agent I'm going to be using and the examples provided by hungerfish but we're going to go into a little bit more details so first we are going to be running looking at this run command all you need to do is simply provide a prompt so in this case The Prompt is generate an image of a boot in the water now depending on all the available tools and models that this agent has access to it will decide which models or agents to use which actually tools or models to use to execute this task so in this case it's just one simple task so it will use a single tool but it can be a multi-layered prompt as well and then in that case it going to be using multiple tools we were going to look at that example in a little bit so here's you actually get an explanation of what's going on so this is coming from the agent the agent says I will use the following tool so since it's a related to image generation so it's going to use the image generator and then the code that is actually executing is image is equal to image generator and the prompt that it's passing on to the image generator is a boat in the water you know it's smart enough to actually ignore the first part of the prompt right because based on the first part it decided which tool or model to use and the actual thing that we want is in the second part of the pumpkin that's why it use the second part so it will simply download the corresponding model that it needs and here is the image that it generated so there is a boot and it's right in in the middle of the water now you can actually do operations on the output that it was generated so for example next we want to create a caption for this image so we took the the output of the last execution so that's the port that is going to be the image right we feed that hint as a input and then we ask the agent to run another prompt or query and this guy this time it is can you caption the uh image that we just generated right and let's see what it it comes up with now in this case it says that I'm going to use image captioner to generate caption for the image so basically it will download either I think it's going to be using a blip model that takes in the image the end generation text description right and the code that it's running is image captioner import that is the caption we're going to look at what different tools that are currently available the the new tools are being added but we're going to look at the current available tools and here is the output so about floating in the ocean now in this case and the on the input that it received was this image but not the initial text prompt that we gave okay uh so far we simply look at single stage prompt execution but let's look at this can you create an image of a boat please read out the contents of the image afterwards so it's supposed to First create an image then it's supposed to create the description of the image so we need a second stage or a model and then we need a third stage model that will take the text input and convert it doodle into an audio and then we will play that audio so let's execute it and let's see what different steps it comes up with as I said before at the agent itself decides which tools to use and Which models to use right so it simply looks at this natural language description and based on this it will decide all the steps so here are the steps I'll be using the following tools so image generator to generate an image the port then text reader to read out loud the contents of the image right then here is how it's going to be done so first generate the image then generate the caption so you get the text description and then you convert it into speech uh this is amazing to think of uh how this is being done uh right now we're using open AI but you can literally integrate any other large language model with this so let's see what it came up with a boat is floating in the water that's pretty cool and the best part is that the caption it generated this text the caption is a port is floating in the water but to text to speech conver text to speech model it only passed this the board is floating in the water so it's pretty neat it can actually understand which is the most important bit so far the tasks that we ask red to execute we actually describe them step by step in our prompt however this works great when the query implies and the use of tools which haven't been described directly so like as an example the read out loud the summary of huggingface.com right so now it has to deduce by itself what type of tools to use and the agent automatic celebs the tool appropriate for the task you want to perform the agent is saying I will use the following tools for the text downloader so first you need to actually access this website and download the text then summarize it to create a summary of the text and then text reader to read it out loud right and the actual tools or models that you use so it's using a text downloader then summarization and then in audio uh text to speech right so hugging face is an eye community building the future more than now organizations are using hugging faces ichat models The Hub is open to all missa models and has support from libraries like flair asteroid etsnet and piano okay so you saw that the text to speech didn't really do a good job with let's say 5000 or AI but you can actually replace whatever that model is when another model I'm going to show you how these tools are defined later in the video run is one command that works on the agent uh and it doesn't keep memory across runs but performs better for multiple operations at once for example if your prompt is using two or more tools or models then it it will run through you all right but there's another command that you can execute and this is called chat so it keeps memory across across runs but performs better at a single instruction now in order to chat all you need to do is just call Dot chat on your agent so let's run it like here my prompt is show me pen image of a car actually I have an extra and but let's see how good this can work here it's it's passing a prompt of a car to the image generator so I think it will work now in the process it will download the model that it needs so here it generated an image of a car now you can continue the chat so it will assume that you were working with the same system so for example if you can ask it transform the image so that it can snow like the initial prompt was showing this animal and now it seems like it's snowing right and you can even execute other operations on top of that so for example this was show me a mask now the chat is great if you want to keep the history of the prompts that you have used so far in order to con start a new chat you will simply execute this function prepare for new chat and it will clean up the history of the previous chat and you can start over with a new chat let's look at which tools are available so there is a tool for a document question answering such as PDF files and the model that's used is called donut then for text question answering it's using flat D5 for unconditional image captioning uh and it's using the blip and then there are other models related to let's say image segmentation text to speech speech to text and so on and so forth there are some Community Based tools as well for example the text downloader from a website that's actually a community too right so uh this list of tools will keep expanding uh it was the initial release so expect this is going to grow a lot this is very similar to the plugins for gpt4 it also provided a section on what if you want to add a new tool yourself right so here is an example of image reading right so there's an API actually which I wasn't aware where you can make a call and you will get a random image of a cat so let's assume you wanted to create a tool around it which whenever is used will call that API in climate of a cat right so here is how it's going to look like uh you are going to be using the tool object from Transformer and you define a class and there are two very important things that you need to consider when you create a user two so first is the name of the tool and then a concise description of what this tool actually does now this description is going to be used by the agent to decide whether to use this tool for a specific task then you simply Define the input and outputs and when this tool is called what operations are going to be performed so here is an example like if you simply use an object of that function of that class so you will get an image now whatever agent you're using so you can pass a list of tools in let's say in this specific case we are passing that one specific two right so based on the prompt the agent will decide which tool to use so it says fit an image of a cat on height and caption it for me so it will look at the description of all the available tools so if I sample in this case it picks this tool to be the most appropriate because it says Fetch and actual image of a cat online here it use that specific tool to get the image and then use the image captioner tool to create a caption for that too caption for that image this also brings us to the importance of the prompt that you are providing so you want to make sure that the prompt is very clear so that the agent can actually decide on which tools to use for to execute your prompt so that is way they actually have a whole section on custom tools and prompts guidelines on their website for example uh the the a shorter version of The Run prompt that is being used so uh it says I will ask you to perform a task right your job is to come up with a series of simple commands right and then it's given a list of tools then a task an example answer right so that's how the master prompt is defined now they have given a couple of good examples of how to get the most out of your prompts so for example if you simply tell it show me a tree right and it's the hacking phase agent so the steps it came up with is I'm going to use an image the segmenter now the reason is this prompt is not very clear based on the tools available and the description so in this case they looked at image generator its description so this the description says this is a tool that creates an image according into a prompt which is a text description right so it takes in input name prompt which contains the description of the image and outputs and image right now you want your prompt to have some words used within the descriptor description of your tool so let's say if you update our prompt to create an image of a tree right so now it has the word image in there right and it's a lot more uh clear to the agent and as a result you see that it's actually using image generator tool now now you can also customize the description of your tools even the ones that are already available so this gives you the ability to customize them to your own prompts which is I think a really great feature to have now you can also customize your prompts and using custom tools we already covered this it's an extremely powerful model or agent that you can use it in your own operations like you can build something like r2gbt on top of this you know a couple of Channel related announcements so now we have a Discord server so come and join us over there quite a few people are already here let's learn from each other link is in the description of the video as always if you have any questions or comments put them in the comment section below if you are new here please consider subscribing to the channel and if you are a subscriber make sure that you press that Bell notification button hope you learned something new today thanks for watching see you in the next one
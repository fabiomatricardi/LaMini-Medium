TITLE: Opensource Supremacy? | OpenLLAMA: The Revenge of Opensource?
video Duration: 0:05:17
----------------------------------------
this is open llama it is an open source reproduction of meta ai's popular llama large language models unlike the original this release is also permissively licensed it is being developed by these two researchers at Berkeley AI research to understand why this work is important we need to travel back in time to the old days of AI to precisely the 25th of February 2023 a little over two months ago it was a good day and the unlikun the true ring price winner and AI honcho at meta AI sent this tweet announcing to the world the release of the Llama large language models the research paper and the code then he added these two more tweets for context and the a world was transformed it was now clear that he was talking about a game changer here's why this were the first models to train on trillions of tokens using only openly available data the model architecture was also new they had longer context length of at least 1496 tokens as shown in this table they clearly beat the state of the art on this standard performance benchmarks llama 32 billion parameter outperformed GPT 3 on most benchmarks despite being 10 times smaller llamas 65 billion parameter is also competitive with the state-of-the-art such as chinchilla with 70 billion parameter and palm with 540 billion parameter and the training loss curves showed that the models could still improve by trailing beyond the 1.4 trillion tokens some people reading the Tweet saw just one problem it wasn't the data all the language models themselves no it was the license which did not permit commercial use as shown here you also had to request access to get the weight so these conditions rubbed a lot of none researches the wrong way and for a while Jan lacun's Twitter streams looked like the hotline for disgruntled teenagers and so with the fullness of time and a glim of Hope emerged across the Atlantic in Europe a Consortium of AI researchers known as together had been doing open source the traditional way and now they focused on reproducing the Llama data set they carefully prepare the source data cleaned it and closely replicated the reported llama data set and on April 17 2023 they released 1.2 trillion token deficit as red pajama this is how it Compares with the Llama data set and this brings us back to the present to open llama the researchers at Berkeley AI research have taken the red pajama dataset and are in the process of reproducing the published research on the Llama large language models they have written that they are using the same architecture the same pre-processing steps and the same hyper parameters only the training data is replaced by the red pajama data set this release is a preview release of the 7 billion parameter model trained on only 200 billion tokens of data which is only one-fifth of the total for the 7 billion parameter model they have also published this preliminary evaluation which looks promising however the training is only about 20 percent done so apart from quality control and project documentation further conclusions made from these Benchmark results can only be temporary about open Llama training loss curve I noticed the flattening of the learning rate after training for 200 billion parameters this line seems almost horizontal while it is likely an artifact of the plotting scale or something else I'm comparing it to this loss curve in the Llama paper where at 200 billion Tokens The Lost curve still had a clear downward slope this slope LED them to speculate that further training was very likely possible well beyond the 1.4 trillion tokens and now let's look at the future of open Llama indeed the future looks bright as you can see here on the chat language model tracker the Llama models are highlighted in yellow and they are the most popular I must say I'm looking forward to open Llama becoming a free alternative model for all these chat language models that now use llama but also keep releasing their llm research code and weights to the world to help the world become a better place thank you
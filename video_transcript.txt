TITLE: LLaMa GPTQ 4-Bit Quantization. Billions of Parameters Made Smaller and Smarter. How Does it Work?
video Duration: 0:11:02
----------------------------------------
hey YouTube today we're going to be going over another powerful tool in our toolbox for large language models gptq 4-bit quantization we're going to be going over the mathematics of how it works and finally how to use it with gptq for llama now if you'd like to skip the mathematics you can skip ahead to the chapter on using gptq for llama but if you'd like to stay for the mathematics we'll be going over immersion features what is quantization uh the differential geometry behind it and finally how to implement 4-bit quantization so let's get started so before we get into four big quantization let's review a few Concepts so let's start with what neural networks are and why quantization is important to them so neural networks are made up of neurons and weights that connect those neurons and these weights can be represented in a matrix where weight one one through weight in M can be represented in that Matrix so if we look at the input in the hidden layer for this example we see that we have a three by three fully connected Network and that Network can be represented by a three by three Matrix and let's say the values in The Matrix are these weights so what we would like to do is is to change these weights from 32-bit floating points to eight or four bit integers making the network for eight times smaller and so how we can do that is through a process called quantization where zero point quantization is probably the most common while there are other techniques so how we do this with 8-bit is we would like to confine the weights in this Matrix between negative 127 and positive 127 and how we do this is we first find the highest value in our weight Matrix which in this case is 9.17 so we take this 9.17 and divide 127 by it and we end up with about 13.84 and we call this our scaling Factor so we take the scaling factor and multiply every value on the matrix by it and then round to the nearest integer so for example if we take 0.84 we would multiply this times our scaling factor of 13.84 and we get roughly 11.6 which rounds to 12. so now let's move on to the next concept so the next thing we need to know is a concept from calculus called the derivative now if you haven't had calculus the only thing that's important to understand here is that this is measuring the rate of change of something how quickly is it changing and in this case if we have some function that describes our position we can know our velocity because that's the rate of change in position and we can know our acceleration because that's the rate of change of our velocity and how we typically notate this is what this dydx form or F Prime of X where the dash means Prime and this is called our first derivative but we can also have second derivatives so if we go back to our position function we can skip velocity and go straight to acceleration and this is notated as this two superscript and subscript notation or F double Prime of x now these derivatives are with respect to single variables but we can also have partial derivatives which let us look at multi-variable functions so if we have some f of x of y and z then we can take a derivative with respect to one of the variables while holding the others fixed so we can look at how only that variable is changing without updating the others and why this is important is this plays into the Hessian which is used to help update our weights for 4-bit quantization and really the most important concept here is that it allows us to look at how the loss is updating so what is the error in our Network as we're updating our weights and that allows us to minimize that loss and we'll see how this plays into 4-bit Quant once we get there but now we can talk about immersion features so the last thing we need to know before 4-bit quantization is emergent features and this is a phenomena of large neural networks where various layers are agreeing on how to label certain features from the input and this can be thought of as how the network embeds knowledge within itself and so we want to be careful that during the quantization process we're not accidentally overwriting these emergent features so we need to know how to find them and they typically present themselves as being weights in a layer that are significantly larger than the what rest of the weights in that layer so for example if we look at column three here we see that these weights are significantly larger than the rest of the weights so we would want to leave these as floating Point values and handle the rest as monetization so what happens is we end up with two sets of values we have one Matrix where we're going ahead and quantizing the values in two integers but then we have another Matrix where we leave them as floating Point values and we leave them as these kind of larger representations and that allows us to preserve the emergent behavior of the network while also getting the benefit of the quantitization so now we can move on to how do we actually do four bit quantization our goal with 4-bit quantization is to minimize the loss function or otherwise the error that we're introducing during this quantitization process and to compute the best possible parameters for this quantization so to do this we're going to follow the following pseudo code we're going to extract the weights for all of our layers and then process them layer by layer so this is a layer-wise quantization process so we're not doing all the layers as a batch rather we're doing them one by one and then we're going to find the Hessian for each layer and remember the Hessian tells us how our loss or error is affected by updating these weights but the Hessian does have a small problem and that it tends to overfit a network so we'd like to introduce a small dampening factor to ensure that we're not overfitting the network and then after doing the dampening Factor we're going to find the inverse Hessian and why we do that is we want to understand the loss sensitivity for values as we're updating these weights so higher values are less sensitive and lower values are more sensitive and you'll see why in just a moment so now we're going to start processing our weights and quantizing them and to do that we're going to pick the first weight in our layer and quantize it using zero point quantization just like we saw with 8-bit quantization but this time with 4-bit but this is where the inverse Hessian comes into play so we're going to compute our total error by taking the original weight and subtracting out the quantized version but then dividing by the relevant value in our inverse Hessian for this particular weight and like we mentioned before the higher the value the less sensitive it is so the smaller the error but the smaller the value the higher the error that we've introduced into the network so what we'd like to do now is balance out this error that we've introduced by going back through the rest of the unquantized weights and adjusting them by the error that we've introduced so for example if we look at weight 1M we would update it to be this weight one M would equal weight one m Plus this error times its relevant value and the Hessian and remember in this case if it's a larger value we can adjust this weight more without having to worry about introducing error and for smaller values we need to adjust it more we need to adjust it less so we would once we finish this we will start quantizing the rest of the values and repeating this process until we have quantized all of our values and the end result of this is generally a more stable Network especially in this kind of mixed model where we're only updating the values that aren't emergent and leaving the emergent values and their floating Point representation now we can let's move on to using a llama library to do GPT q4-bit quantization on a llama model now if you'd like to be able to do gptq 4-bit quantization on llama models we first have to check out the gptq for llama repository and that will be in the description below so once that's downloaded we're going to CD into the directory for it and we're going to install the requirements for it easy to pip and that takes just a little bit of time now if you're a Windows user like I am you you'll have to use WSL unless you're willing to build the dependencies manually I wasn't so I just used WSL now once we have everything installed now we can do the 4-bit quantization so we have to have an unquantized model that we plan to use and I already have that so let's go ahead and go through this command and see what it does so we're going to call Llama Pi we're going to tell it where our model is and we're going to tell it the number of bits in this case we plan to do 4-bit quantization we want to use true sequential we want to use act order we want to use group size 128 and then we're going to tell it where to save it in my case output.pt and this command line will also be in the description though it's also in the repo um so now we can go ahead and run this command and this will take a significant amount of time to run but the 7 billion and 13 billion parameter models should be able to run in a variety of systems and I'll also try to include the requirements for this in the description as well now I'm going to go ahead and let this run and I'll come back once it's done so once this is done processing you should see the following and I kind of wanted to show y'all what you should see as it is working through quantizing the weights you're going to see a lot of printouts for these layers as they update and if you see any really high weight errors you should probably think that there are some issues going on but other than that as long as you use the default settings this should just run without any pain it takes about 20 to 25 minutes on my machine your results will likely vary and that's it if this was helpful for you please like And subscribe and let us know in the comments what you'd like to learn about next and stay tuned for our continued conversation about how large language models work